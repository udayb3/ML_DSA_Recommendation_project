{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9280\\2652107114.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import marshal\n",
    "import types\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Function to get embedding for a new input question\n",
    "def get_question_embedding(text, mtokenizer, mmodel, device):\n",
    "    inputs = mtokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = mmodel(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Apply average pooling\n",
    "        sentence_embedding = last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy()\n",
    "    return sentence_embedding\n",
    "\n",
    "def find_similar_questions(input_text, mtokenizer, mmodel,  pca_class, my_cosine, embed, device, train_sample, top_n=5):\n",
    "\n",
    "  input_embedding = get_question_embedding(input_text, mtokenizer, mmodel, device)\n",
    "\n",
    "  # Reduce the dimensionality of the input embedding to match the stored embeddings\n",
    "  input_embedding_reduced = pca_class.transform(input_embedding.reshape(1, -1))\n",
    "\n",
    "  # Calculate cosine similarity with each question embedding in train_sample\n",
    "  similarities = my_cosine(input_embedding_reduced, embed)\n",
    "\n",
    "  # Get the top N most similar questions\n",
    "  top_indices = similarities[0].argsort()[-top_n:][::-1]\n",
    "  similar_questions = train_sample.iloc[top_indices][['name']]\n",
    "  return similar_questions\n",
    "\n",
    "# Device= 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "my_model = pickle.load(open(\"./../../../pre_trained_model/embedding/embed_model.pkl\", 'rb'))\n",
    "my_tokenizer= pickle.load( open(\"./../../../pre_trained_model/embedding/embed_tokenizer.pkl\", 'rb') )\n",
    "my_cosine= pickle.load(open(\"./../../../pre_trained_model/embedding/embed_cosine.pkl\", 'rb') )\n",
    "my_pca= pickle.load(open(\"./../../../pre_trained_model/embedding/embed_pca.pkl\", 'rb') )\n",
    "my_embed= pickle.load(open(\"./../../../pre_trained_model/embedding/embed_embeddings.pkl\", 'rb') )\n",
    "my_train_sample= pickle.load(open(\"./../../../pre_trained_model/embedding/embed_train_sample.pkl\", 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>Divide Intervals Into Minimum Number of Groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>Longest Subsequence Repeated k Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>Remove Stones to Minimize the Total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>Minimum Subsequence in Non-Increasing Order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>Minimum Initial Energy to Finish Tasks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>Minimum Interval to Include Each Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>Design a Food Rating System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>Replace Question Marks in String to Minimize I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>Minimum Difference in Sums After Removal of El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>Maximum Score From Removing Stones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>Difference Between Ones and Zeros in Row and C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Course Schedule III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>Time Taken to Mark All Nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>Probability of a Two Boxes Having The Same Num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Roman to Integer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>Check if Strings Can be Made Equal With Operat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>Minimize Manhattan Distances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>Find the Kth Smallest Sum of a Matrix With Sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>Get Biggest Three Rhombus Sums in a Grid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>Execution of All Suffix Instructions Staying i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name\n",
       "1791     Divide Intervals Into Minimum Number of Groups\n",
       "1510               Longest Subsequence Repeated k Times\n",
       "1472                Remove Stones to Minimize the Total\n",
       "1062        Minimum Subsequence in Non-Increasing Order\n",
       "1252             Minimum Initial Energy to Finish Tasks\n",
       "1392             Minimum Interval to Include Each Query\n",
       "1754                        Design a Food Rating System\n",
       "2281  Replace Question Marks in String to Minimize I...\n",
       "1619  Minimum Difference in Sums After Removal of El...\n",
       "1321                 Maximum Score From Removing Stones\n",
       "1852  Difference Between Ones and Zeros in Row and C...\n",
       "472                                 Course Schedule III\n",
       "2397                       Time Taken to Mark All Nodes\n",
       "1106  Probability of a Two Boxes Having The Same Num...\n",
       "10                                     Roman to Integer\n",
       "2114  Check if Strings Can be Made Equal With Operat...\n",
       "2298                       Minimize Manhattan Distances\n",
       "1087  Find the Kth Smallest Sum of a Matrix With Sor...\n",
       "1411           Get Biggest Three Rhombus Sums in a Grid\n",
       "1588  Execution of All Suffix Instructions Staying i..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_question = \"three sum\"\n",
    "find_similar_questions(input_question, my_tokenizer, my_model, my_pca, my_cosine, my_embed, 'cpu',my_train_sample,  20 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
