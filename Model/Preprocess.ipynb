{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2427 entries, 0 to 2426\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          2427 non-null   int64 \n",
      " 1   name        2427 non-null   object\n",
      " 2   text        2427 non-null   object\n",
      " 3   accecpted   2427 non-null   int64 \n",
      " 4   submission  2427 non-null   int64 \n",
      " 5   topics      2427 non-null   object\n",
      " 6   difficulty  2427 non-null   object\n",
      " 7   link        2427 non-null   object\n",
      " 8   website     2427 non-null   object\n",
      " 9   upvotes     2427 non-null   int64 \n",
      " 10  comments    2427 non-null   int64 \n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 227.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_json('../data_collect/website1/gen_data.json')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df= df[['id', 'name', 'text', 'topics', '']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/udayb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/udayb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m questions\u001b[38;5;241m=\u001b[39m deepcopy(df)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(questions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 53\u001b[0m   questions\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[43mBasic_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mBasic_preprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m     36\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m(word \u001b[38;5;129;01min\u001b[39;00m inco_spell):\n\u001b[0;32m---> 37\u001b[0m \t\tnew_word\u001b[38;5;241m=\u001b[39m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m \t\tnew_word\u001b[38;5;241m=\u001b[39m word\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/spellchecker/spellchecker.py:148\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" The most probable correct spelling for the word\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m        word (str): The word to correct\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m        str: The most likely candidate \"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m word \u001b[38;5;241m=\u001b[39m ENSURE_UNICODE(word)\n\u001b[0;32m--> 148\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28msorted\u001b[39m(candidates), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_probability)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/spellchecker/spellchecker.py:173\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/spellchecker/spellchecker.py:262\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    256\u001b[0m words \u001b[38;5;241m=\u001b[39m [ENSURE_UNICODE(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    257\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    258\u001b[0m     w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)\n\u001b[1;32m    261\u001b[0m ]\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/spellchecker/spellchecker.py:188\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        set: The set of those words from the input that are in the \\\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m        corpus \"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m words \u001b[38;5;241m=\u001b[39m [ENSURE_UNICODE(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m--> 188\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    190\u001b[0m     w\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)\n\u001b[1;32m    194\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "math_pattern = r'\\b[\\d\\+\\-\\*/=\\(\\)\\s]+\\b'\n",
    "addit= ['given', 'follow', 'exampl', 'consist', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero']\n",
    "for nw in addit:\n",
    "\tstop_words.add(nw)\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "# Text Preprocessing\n",
    "def Basic_preprocess(text):\n",
    "\ttext = text.lower()\n",
    "\ttext = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  \n",
    "\t# tokenizing the text\n",
    "\ttokens = word_tokenize(text)\n",
    "\t\n",
    "\t# initializing the spell checker\n",
    "\tspell = SpellChecker()\n",
    "\tinco_spell= spell.unknown(tokens);\tfinal_tokens= \"\"\n",
    "\n",
    "\t# using spell checker to correct the spelling of the word\n",
    "\tfor word in tokens:\n",
    "\n",
    "\t\tif(word in inco_spell):\n",
    "\t\t\tnew_word= spell.correction(word)\n",
    "\t\telse:\n",
    "\t\t\tnew_word= word\n",
    "\n",
    "\t\tif(new_word not in stop_words ):\n",
    "\t\t\ttemp= re.sub(r'\\d+', '', new_word)\n",
    "\t\t\ttemp = re.sub(math_pattern, '', temp)\n",
    "\n",
    "\t\t\tif(temp!=''):\n",
    "\t\t\t\tnew_word= stemmer.stem(word)\n",
    "\t\t\t\tfinal_tokens+= new_word + \" \"\n",
    "\n",
    "\treturn final_tokens\n",
    "\n",
    "questions= deepcopy(df)\n",
    "for i, row in enumerate(questions['text']):\n",
    "  questions.loc[i, 'text']= Basic_preprocess(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
